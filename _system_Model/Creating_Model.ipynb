{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Keyword Spotting Model For Voice Assistant \"Numa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Library\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "import wave \n",
    "import struct\n",
    "\n",
    "# Audio Data Analysis Library\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization Library\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Reading and Exploring Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mappings</th>\n",
       "      <th>labels</th>\n",
       "      <th>MFCCs</th>\n",
       "      <th>files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-757.322509765625, -2.326998710632324, -0.86...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-354.143798828125, 94.80502319335938, -2.048...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chrome</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-820.5107421875, 19.644493103027344, 4.52003...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2.370381116867065, 55.97502899169922, 5.4160...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>background_noise</td>\n",
       "      <td>0</td>\n",
       "      <td>[[34.62870407104492, 96.85321807861328, 6.8009...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>folder</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-36.8433837890625, 129.1125030517578, -35.90...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>google</td>\n",
       "      <td>0</td>\n",
       "      <td>[[163.28610229492188, 71.62339782714844, -21.8...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>numa</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-768.443603515625, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shutdown</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-855.1250610351562, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>computer_noise</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-413.906005859375, 172.31414794921875, -4.17...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>play</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-978.7564697265625, -9.972305297851562, 8.94...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-373.03045654296875, 148.76028442382812, -29...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-829.8956298828125, 66.89012145996094, 66.90...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-840.1062622070312, 6.9649152755737305, -38....</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-361.4137268066406, 99.54976654052734, -11.7...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-832.75146484375, 44.155982971191406, -14.18...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-433.71246337890625, 166.43539428710938, -27...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-871.768310546875, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-773.2343139648438, 95.66471862792969, 11.54...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-892.963623046875, 12.419766426086426, 12.72...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mappings labels  \\\n",
       "0               file      0   \n",
       "1               open      0   \n",
       "2             chrome      0   \n",
       "3              close      0   \n",
       "4   background_noise      0   \n",
       "5             folder      0   \n",
       "6             google      0   \n",
       "7               numa      0   \n",
       "8           shutdown      0   \n",
       "9     computer_noise      0   \n",
       "10              play      0   \n",
       "11           youtube      0   \n",
       "12              None      0   \n",
       "13              None      0   \n",
       "14              None      0   \n",
       "15              None      0   \n",
       "16              None      0   \n",
       "17              None      0   \n",
       "18              None      0   \n",
       "19              None      0   \n",
       "\n",
       "                                                MFCCs  \\\n",
       "0   [[-757.322509765625, -2.326998710632324, -0.86...   \n",
       "1   [[-354.143798828125, 94.80502319335938, -2.048...   \n",
       "2   [[-820.5107421875, 19.644493103027344, 4.52003...   \n",
       "3   [[2.370381116867065, 55.97502899169922, 5.4160...   \n",
       "4   [[34.62870407104492, 96.85321807861328, 6.8009...   \n",
       "5   [[-36.8433837890625, 129.1125030517578, -35.90...   \n",
       "6   [[163.28610229492188, 71.62339782714844, -21.8...   \n",
       "7   [[-768.443603515625, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8   [[-855.1250610351562, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "9   [[-413.906005859375, 172.31414794921875, -4.17...   \n",
       "10  [[-978.7564697265625, -9.972305297851562, 8.94...   \n",
       "11  [[-373.03045654296875, 148.76028442382812, -29...   \n",
       "12  [[-829.8956298828125, 66.89012145996094, 66.90...   \n",
       "13  [[-840.1062622070312, 6.9649152755737305, -38....   \n",
       "14  [[-361.4137268066406, 99.54976654052734, -11.7...   \n",
       "15  [[-832.75146484375, 44.155982971191406, -14.18...   \n",
       "16  [[-433.71246337890625, 166.43539428710938, -27...   \n",
       "17  [[-871.768310546875, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "18  [[-773.2343139648438, 95.66471862792969, 11.54...   \n",
       "19  [[-892.963623046875, 12.419766426086426, 12.72...   \n",
       "\n",
       "                                                files  \n",
       "0   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "1   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "2   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "3   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "4   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "5   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "6   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "7   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "8   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "9   /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "10  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "11  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "12  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "13  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "14  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "15  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "16  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "17  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "18  /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "19  /home/atomyongya/Documents/Herald/Final Year P...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creating dataframe of json file.\n",
    "\n",
    ":param orginal_df (DataFrame) : Dataframe of json data. \n",
    ":param df : Copy file of orginal_df.\n",
    "\"\"\"\n",
    "\n",
    "json_Path_English = \"/home/atomyongya/Documents/Herald/Final Year Project/VoiceAssistant(Numa)/VoiceAssistant/_system_Model/2_English_KM/2_English_Json_Output_File/English_Data_JSON.json\"\n",
    "\n",
    "orginal_df = pd.read_json(json_Path_English, orient='index')\n",
    "df = orginal_df\n",
    "\n",
    "\"\"\"\n",
    "Applying transpose() function which changes the row elements into column \n",
    "elements and the column elements into row elements.\n",
    "\"\"\"\n",
    "\n",
    "df = df.transpose()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappings    1572\n",
       "labels         0\n",
       "MFCCs          0\n",
       "files          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding null value\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1584, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['file', 'open', 'chrome', 'close', 'background_noise', 'folder',\n",
       "       'google', 'numa', 'shutdown', 'computer_noise', 'play', 'youtube',\n",
       "       None], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique class of \"mappings\" feature.\n",
    "mapping_Unique_Class = df[\"mappings\"].unique()\n",
    "mapping_Unique_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique class of \"labels\" feature\n",
    "labels_Unique_Class = df[\"labels\"].unique()\n",
    "labels_Unique_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1584"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique class of \"files\" feature\n",
    "files_Unique_Class = df[\"files\"].unique()\n",
    "len(files_Unique_Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mappings</th>\n",
       "      <th>labels</th>\n",
       "      <th>MFCCs</th>\n",
       "      <th>files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12</td>\n",
       "      <td>1584</td>\n",
       "      <td>1584</td>\n",
       "      <td>1584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1525</td>\n",
       "      <td>1584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>file</td>\n",
       "      <td>7</td>\n",
       "      <td>[[-676.4949951171875, 85.03929138183594, 5.804...</td>\n",
       "      <td>/home/atomyongya/Documents/Herald/Final Year P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mappings  labels                                              MFCCs  \\\n",
       "count        12    1584                                               1584   \n",
       "unique       12      12                                               1525   \n",
       "top        file       7  [[-676.4949951171875, 85.03929138183594, 5.804...   \n",
       "freq          1     201                                                  2   \n",
       "\n",
       "                                                    files  \n",
       "count                                                1584  \n",
       "unique                                               1584  \n",
       "top     /home/atomyongya/Documents/Herald/Final Year P...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1584 entries, 0 to 1583\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   mappings  12 non-null     object\n",
      " 1   labels    1584 non-null   object\n",
      " 2   MFCCs     1584 non-null   object\n",
      " 3   files     1584 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 49.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lode json file.\n",
    "\n",
    ":param data (dict) : Dictonary which store the JSON data. \n",
    "\"\"\"\n",
    "\n",
    "with open(json_Path_English) as json_Data:\n",
    "    data = json.load(json_Data)\n",
    "\n",
    "# pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing audio data before preparing dataset to get more information of our audio data.\n",
    "\n",
    "class Meta_Data():\n",
    "    \"\"\"\n",
    "    The information from class \"Meta_Data\" will be used later to create a model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, audio_Data_Path):\n",
    "        \"\"\"\n",
    "        :param audio_Data_Path : path of audio data.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.path = audio_Data_Path\n",
    "    \n",
    "    # Method to play audio\n",
    "    def play_Audio(self):\n",
    "        \"\"\"\n",
    "        Playing Audio to make sure we are using the correct input to extract MFCCs.\n",
    "        \n",
    "        :param audio : audio file in .wav format.\n",
    "        \"\"\"\n",
    "        \n",
    "        audio = ipd.Audio(self.path)\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    # Method to Extract MFCCs\n",
    "    def extracting_MFCCs(self):\n",
    "        \"\"\"\n",
    "        MFCCs (Mel Frequency Cepstrum Coefficent) is representation of the short-term power spectrum of an audio or sound.\n",
    "        \n",
    "        :param signal : \n",
    "        :param sample_Rate : Number of sample in one second.\n",
    "        :param n_mfcc : Number of coefficent (y-intercept).\n",
    "        :paramm mfccs: Feature of audio data in 2D array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Loading audio files\n",
    "        signal, sample_Rate = librosa.load(self.path)\n",
    "        \n",
    "        # Extracting MFCCs Feature\n",
    "        mfccs = librosa.feature.mfcc(y=signal, n_mfcc=13, sr=sample_Rate)\n",
    "        print(\"Shape of an audio: \", mfccs.shape)\n",
    "        \n",
    "        return signal, sample_Rate, mfccs\n",
    "    \n",
    "    # Method to Visualise MFCCs in json\n",
    "    def visualising_MFCCs(self):\n",
    "        signal, sample_Rate, mfccs = self.extracting_MFCCs()\n",
    "        plt.figure(figsize=(25, 10))\n",
    "        librosa.display.specshow(mfccs, x_axis=\"time\", sr=sample_Rate)\n",
    "        plt.title(\"Mel Frequency Cepstrum Coefficent (MFCCs)\")\n",
    "        plt.colorbar(format=\"%+2.f\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class for data visualization from dataframe\n",
    "class Graph_Plot():\n",
    "    \n",
    "    \"\"\"\n",
    "    :param x_Axis : input feature\n",
    "    :param y_Axis : output\n",
    "    \n",
    "    \"\"\"\n",
    "    def it__(self, x_Axis, y_Axis):\n",
    "        self.x_Axis = x_Axis\n",
    "        self.y_Axis = y_Axis\n",
    "\n",
    "    def figure_Size(self): \n",
    "        figure_Size = plt.figure(figsize=(15, 7))\n",
    "\n",
    "    def bar_Plot(self):\n",
    "        self.figure_Size()\n",
    "        sns.barplot(x=self.x_Axis, y=self.y_Axis, data=df)\n",
    "\n",
    "    def box_Plot(self):\n",
    "        self.figure_Size()\n",
    "        plt.boxplot(x=self.x_Axis, data=df)\n",
    "        plt.show()\n",
    "\n",
    "    def swarmp_Plot(self):\n",
    "        self.figure_Size()\n",
    "        sns.swarmplot(x=self.x_Axis, y=self.y_Axis, data=df)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating first object of class Meta_Data randomly\n",
    "# audio_Path = random.choice(data[\"files\"])\n",
    "# random_Audio_Object = Meta_Data(audio_Path)\n",
    "\n",
    "# # Calling play_Audio() method from class Meta_Data for first object\n",
    "# random_Audio_Object.play_Audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# :param name_Of_File : Split the directory and store in list. [-2] Return the second last index value.\n",
    "# \"\"\"\n",
    "# name_of_File = audio_Path.split(\"/\")[-2]\n",
    "# print(name_of_File + \" Audio\\n\")\n",
    "\n",
    "# random_Audio_Object.visualising_MFCCs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating second object of Meta_Data \n",
    "# audio_Path2 = random.choice(data[\"files\"])\n",
    "# random_Audio_Object2 = Meta_Data(audio_Path2)\n",
    "\n",
    "# # Calling play_Audio() method from class Meta_Data for first object\n",
    "# random_Audio_Object2.play_Audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_Path_Name = audio_Path2.split(\"/\")[-2]\n",
    "# print(file_Path_Name + \" Audio\\n\")\n",
    "# random_Audio_Object2.visualising_MFCCs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs = df[\"MFCCs\"]\n",
    "# mapping = df[\"mappings\"]\n",
    "# labels = df[\"labels\"]\n",
    "\n",
    "# mfccs.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Library to create model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating keyword spotting model for speech recognition system.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Create_Model():\n",
    "    \"\"\"\n",
    "    Class to create model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_Path, save_Model_Path, batch_size, epochs, learning_Rate):\n",
    "        \"\"\"\n",
    "        Constructor function.\n",
    "        \n",
    "        :param data_Path : Path of the JSON file.\n",
    "        :param save_Model : Path where .h5 model will be saved.\n",
    "        :param batch_size : Number of sample processed before the model is updated.\n",
    "        :param epochs : one entire transit of the training data through the algorithm.\n",
    "        :param learning_Rate : Hyperparameter of lstm algorithm which decide how training process data will be selected \n",
    "                                due to which the building model can take long or short period of time.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_Path = data_Path\n",
    "        self.save_Model_Path = save_Model_Path\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_Rate = learning_Rate\n",
    "    \n",
    "    def load_Dataset(self, data_Path):\n",
    "        \"\"\"\n",
    "        Loading JSON dataset.\n",
    "        \n",
    "        :var data : Dictionary to store json data after loading.\n",
    "        :var X : Input feature.\n",
    "        :var y : Output.\n",
    "        \"\"\"\n",
    "        # Loading JSON File\n",
    "        with open(data_Path, \"r\") as json_Data:\n",
    "            data = json.load(json_Data)\n",
    "            \n",
    "        # Extract Inputs and Targets (or Labels) Features\n",
    "        X = np.asarray(data[\"MFCCs\"], dtype=object)\n",
    "        y = np.asarray(data[\"labels\"], dtype=object)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def get_Data_Splits(self, data_Path, test_size=0.1, test_validation=0.1):\n",
    "        \"\"\"\n",
    "        Splitting the data in train, test, validation.\n",
    "        \n",
    "        :param test_size : Size of test data from whole dataset.\n",
    "        :param test_validation : Size of validation data from remaining tranning dataset.\n",
    "        \n",
    "        :var X_train : Input training data for model.\n",
    "        :var X_test : Input testing data for model.\n",
    "        :var X_validation : Input Validation data for model.\n",
    "        \n",
    "        :var y_train : Output training data for model.\n",
    "        :var y_test : Output testing data for model. \n",
    "        :var y_validation : Output validation data for model.\n",
    "        \"\"\"\n",
    "        # Load Dataset\n",
    "        X, y = self.load_Dataset(self.data_Path)\n",
    "        \n",
    "        # split train/Validation/test \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=test_validation)\n",
    "        \n",
    "        X_train = tf.convert_to_tensor(X_train, dtype=tf.float32) \n",
    "        X_validation = tf.convert_to_tensor(X_validation, dtype=tf.float32)\n",
    "        X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "        y_train = tf.convert_to_tensor(y_train, dtype=tf.float32) \n",
    "        y_validation = tf.convert_to_tensor(y_validation, dtype=tf.float32)\n",
    "        y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "        \n",
    "        return X_train, X_validation, X_test, y_train, y_validation, y_test \n",
    "    \n",
    "    def build_Model(self, input_shape, learning_Rate, error=\"sparse_categorical_crossentropy\", number_Keywords=12):\n",
    "        \"\"\"\n",
    "        Building the keyspotting model.\n",
    "        \n",
    "        :param error : Algorithm to calculate the error of model.\n",
    "        :param number_Keywords : Number of word we collected for traning purpose.\n",
    "        \n",
    "        :var model : Model type.\n",
    "        :var optimiser : Algorithm that helps to reduce loss or error and increase the accuracy of model.\n",
    "        \"\"\"\n",
    "        # Build network\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        # 2 LSTM Layer\n",
    "        model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "        model.add(keras.layers.LSTM(64))\n",
    "        \n",
    "        # Dense Layer\n",
    "        model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "        \n",
    "        # Softmax classifier (or Output layer)\n",
    "        model.add(keras.layers.Dense(number_Keywords, activation=\"softmax\")) # [0.1, 0.7, 0.2] 0.7 will be output\n",
    "        \n",
    "        # Compile the model\n",
    "        optimiser = keras.optimizers.Adam(learning_rate=self.learning_Rate)\n",
    "        model.compile(optimizer=optimiser, loss=error, metrics=[\"accuracy\"])\n",
    "        \n",
    "        # Print model overview\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Main function from where the process of creating model start.\n",
    "        \n",
    "        :var input_shape : input shape of the first node in neural network.\n",
    "        :var test_error : Error of our model.\n",
    "        :var test_accuracy : Accuracy of our model.\n",
    "        \"\"\"\n",
    "        # train /  validation / test data splits\n",
    "        X_train, X_validation, X_test, y_train, y_validation, y_test = self.get_Data_Splits(self.data_Path)\n",
    "        \n",
    "        # Building LSTM Model\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = self.build_Model(input_shape, self.learning_Rate)\n",
    "        \n",
    "        # Train the data using model\n",
    "        model.fit(X_train, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_data=(X_validation, y_validation))\n",
    "        \n",
    "        # Evaluating the model\n",
    "        test_error, test_accuracy = model.evaluate(X_test, y_test)\n",
    "        print(f\"Test error: {test_error}, Test accuracy: {test_accuracy}\")\n",
    "        \n",
    "        # Saving model\n",
    "        model.save(self.save_Model_Path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating object of class Create_Model for English language.\n",
    "\n",
    ":var json_Path_English : Path of JSON dataset for English language.\n",
    ":var english_Model_Path : Path where english keyspotting model get saved. \n",
    "\"\"\"\n",
    "\n",
    "learning_Rate = 0.0001\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Data Path\n",
    "json_Path_English = \"/home/atomyongya/Documents/Herald/Final Year Project/VoiceAssistant(Numa)/VoiceAssistant/_system_Model/2_English_KM/2_English_Json_Output_File/English_Data_JSON.json\"\n",
    "\n",
    "# Model Path\n",
    "english_Model_Path = \"/home/atomyongya/Documents/Herald/Final Year Project/VoiceAssistant(Numa)/VoiceAssistant/_system_Model/2_English_KM/3_English_Model_File/english_Model.h5\"\n",
    "\n",
    "# Creating model object for English\n",
    "english_Model_Object = Create_Model(json_Path_English, english_Model_Path, batch_size, epochs, learning_Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreating object of class create_Model for Nepali Language.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creating object of class create_Model for Nepali Language.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 44, 64)            19968     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,932\n",
      "Trainable params: 57,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "41/41 [==============================] - 3s 33ms/step - loss: 2.4988 - accuracy: 0.0796 - val_loss: 2.4910 - val_accuracy: 0.0699\n",
      "Epoch 2/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 2.4427 - accuracy: 0.1217 - val_loss: 2.4360 - val_accuracy: 0.1189\n",
      "Epoch 3/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 2.3928 - accuracy: 0.1498 - val_loss: 2.3908 - val_accuracy: 0.1469\n",
      "Epoch 4/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 2.3387 - accuracy: 0.1989 - val_loss: 2.3446 - val_accuracy: 0.1678\n",
      "Epoch 5/100\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 2.2857 - accuracy: 0.2426 - val_loss: 2.2887 - val_accuracy: 0.2727\n",
      "Epoch 6/100\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 2.2199 - accuracy: 0.2691 - val_loss: 2.2157 - val_accuracy: 0.2797\n",
      "Epoch 7/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 2.1167 - accuracy: 0.3471 - val_loss: 2.1258 - val_accuracy: 0.2937\n",
      "Epoch 8/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 2.0425 - accuracy: 0.3409 - val_loss: 2.0286 - val_accuracy: 0.3427\n",
      "Epoch 9/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 1.9428 - accuracy: 0.3775 - val_loss: 1.9268 - val_accuracy: 0.3986\n",
      "Epoch 10/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 1.8274 - accuracy: 0.4095 - val_loss: 1.8072 - val_accuracy: 0.4895\n",
      "Epoch 11/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 1.7269 - accuracy: 0.4353 - val_loss: 1.6929 - val_accuracy: 0.5035\n",
      "Epoch 12/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 1.6119 - accuracy: 0.5055 - val_loss: 1.5680 - val_accuracy: 0.5175\n",
      "Epoch 13/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 1.5260 - accuracy: 0.5133 - val_loss: 1.4784 - val_accuracy: 0.5594\n",
      "Epoch 14/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 1.4236 - accuracy: 0.5632 - val_loss: 1.3693 - val_accuracy: 0.5734\n",
      "Epoch 15/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 1.3177 - accuracy: 0.6061 - val_loss: 1.2688 - val_accuracy: 0.6014\n",
      "Epoch 16/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 1.2596 - accuracy: 0.6225 - val_loss: 1.1887 - val_accuracy: 0.6154\n",
      "Epoch 17/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 1.1821 - accuracy: 0.6482 - val_loss: 1.0916 - val_accuracy: 0.6643\n",
      "Epoch 18/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 1.0911 - accuracy: 0.6966 - val_loss: 1.0184 - val_accuracy: 0.6993\n",
      "Epoch 19/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 1.0300 - accuracy: 0.7114 - val_loss: 0.9441 - val_accuracy: 0.7133\n",
      "Epoch 20/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.9577 - accuracy: 0.7324 - val_loss: 0.9015 - val_accuracy: 0.7273\n",
      "Epoch 21/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.8745 - accuracy: 0.7746 - val_loss: 0.8540 - val_accuracy: 0.7133\n",
      "Epoch 22/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.8639 - accuracy: 0.7629 - val_loss: 0.8227 - val_accuracy: 0.7413\n",
      "Epoch 23/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.7948 - accuracy: 0.7816 - val_loss: 0.7695 - val_accuracy: 0.7762\n",
      "Epoch 24/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.7431 - accuracy: 0.7910 - val_loss: 0.7296 - val_accuracy: 0.7832\n",
      "Epoch 25/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.6972 - accuracy: 0.8144 - val_loss: 0.6785 - val_accuracy: 0.7832\n",
      "Epoch 26/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.6548 - accuracy: 0.8183 - val_loss: 0.6696 - val_accuracy: 0.7832\n",
      "Epoch 27/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.6004 - accuracy: 0.8401 - val_loss: 0.6490 - val_accuracy: 0.7832\n",
      "Epoch 28/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.6251 - accuracy: 0.8183 - val_loss: 0.6064 - val_accuracy: 0.8252\n",
      "Epoch 29/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.5318 - accuracy: 0.8580 - val_loss: 0.5745 - val_accuracy: 0.8182\n",
      "Epoch 30/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4954 - accuracy: 0.8651 - val_loss: 0.5498 - val_accuracy: 0.8322\n",
      "Epoch 31/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.4711 - accuracy: 0.8861 - val_loss: 0.5259 - val_accuracy: 0.8322\n",
      "Epoch 32/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4426 - accuracy: 0.8846 - val_loss: 0.5038 - val_accuracy: 0.8531\n",
      "Epoch 33/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4498 - accuracy: 0.8799 - val_loss: 0.5194 - val_accuracy: 0.8392\n",
      "Epoch 34/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4226 - accuracy: 0.8900 - val_loss: 0.4596 - val_accuracy: 0.8671\n",
      "Epoch 35/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3901 - accuracy: 0.9009 - val_loss: 0.4446 - val_accuracy: 0.8741\n",
      "Epoch 36/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3804 - accuracy: 0.9009 - val_loss: 0.4409 - val_accuracy: 0.8601\n",
      "Epoch 37/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3640 - accuracy: 0.9134 - val_loss: 0.4431 - val_accuracy: 0.8601\n",
      "Epoch 38/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3289 - accuracy: 0.9259 - val_loss: 0.4411 - val_accuracy: 0.8741\n",
      "Epoch 39/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3414 - accuracy: 0.9025 - val_loss: 0.4204 - val_accuracy: 0.8881\n",
      "Epoch 40/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3168 - accuracy: 0.9150 - val_loss: 0.4186 - val_accuracy: 0.8601\n",
      "Epoch 41/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2992 - accuracy: 0.9220 - val_loss: 0.3927 - val_accuracy: 0.8741\n",
      "Epoch 42/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2713 - accuracy: 0.9360 - val_loss: 0.3959 - val_accuracy: 0.8601\n",
      "Epoch 43/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2682 - accuracy: 0.9282 - val_loss: 0.3929 - val_accuracy: 0.8741\n",
      "Epoch 44/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2632 - accuracy: 0.9267 - val_loss: 0.4045 - val_accuracy: 0.8601\n",
      "Epoch 45/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2528 - accuracy: 0.9353 - val_loss: 0.3896 - val_accuracy: 0.8741\n",
      "Epoch 46/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.2524 - accuracy: 0.9345 - val_loss: 0.3799 - val_accuracy: 0.8811\n",
      "Epoch 47/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.2465 - accuracy: 0.9321 - val_loss: 0.3563 - val_accuracy: 0.8951\n",
      "Epoch 48/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.2392 - accuracy: 0.9345 - val_loss: 0.4113 - val_accuracy: 0.8811\n",
      "Epoch 49/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.2310 - accuracy: 0.9384 - val_loss: 0.3724 - val_accuracy: 0.8951\n",
      "Epoch 50/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2161 - accuracy: 0.9407 - val_loss: 0.3644 - val_accuracy: 0.8741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.2068 - accuracy: 0.9470 - val_loss: 0.3503 - val_accuracy: 0.8741\n",
      "Epoch 52/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1914 - accuracy: 0.9532 - val_loss: 0.3327 - val_accuracy: 0.9021\n",
      "Epoch 53/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1875 - accuracy: 0.9485 - val_loss: 0.3555 - val_accuracy: 0.8811\n",
      "Epoch 54/100\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.1895 - accuracy: 0.9485 - val_loss: 0.3969 - val_accuracy: 0.8881\n",
      "Epoch 55/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1941 - accuracy: 0.9485 - val_loss: 0.3291 - val_accuracy: 0.9021\n",
      "Epoch 56/100\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.1717 - accuracy: 0.9587 - val_loss: 0.3270 - val_accuracy: 0.9091\n",
      "Epoch 57/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1744 - accuracy: 0.9485 - val_loss: 0.3492 - val_accuracy: 0.8881\n",
      "Epoch 58/100\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.1527 - accuracy: 0.9571 - val_loss: 0.3382 - val_accuracy: 0.8951\n",
      "Epoch 59/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1586 - accuracy: 0.9563 - val_loss: 0.3302 - val_accuracy: 0.9161\n",
      "Epoch 60/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1561 - accuracy: 0.9610 - val_loss: 0.3493 - val_accuracy: 0.9091\n",
      "Epoch 61/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1607 - accuracy: 0.9548 - val_loss: 0.3223 - val_accuracy: 0.9021\n",
      "Epoch 62/100\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.1577 - accuracy: 0.9555 - val_loss: 0.2899 - val_accuracy: 0.9091\n",
      "Epoch 63/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.1419 - accuracy: 0.9594 - val_loss: 0.3184 - val_accuracy: 0.9021\n",
      "Epoch 64/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.1325 - accuracy: 0.9633 - val_loss: 0.3133 - val_accuracy: 0.9091\n",
      "Epoch 65/100\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.1276 - accuracy: 0.9618 - val_loss: 0.3061 - val_accuracy: 0.9161\n",
      "Epoch 66/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1256 - accuracy: 0.9688 - val_loss: 0.3659 - val_accuracy: 0.8741\n",
      "Epoch 67/100\n",
      "41/41 [==============================] - 1s 31ms/step - loss: 0.1194 - accuracy: 0.9750 - val_loss: 0.3574 - val_accuracy: 0.9021\n",
      "Epoch 68/100\n",
      "41/41 [==============================] - 1s 28ms/step - loss: 0.1247 - accuracy: 0.9641 - val_loss: 0.3236 - val_accuracy: 0.9021\n",
      "Epoch 69/100\n",
      "41/41 [==============================] - 1s 28ms/step - loss: 0.1261 - accuracy: 0.9657 - val_loss: 0.3139 - val_accuracy: 0.9091\n",
      "Epoch 70/100\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.1103 - accuracy: 0.9719 - val_loss: 0.3160 - val_accuracy: 0.9091\n",
      "Epoch 71/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1059 - accuracy: 0.9719 - val_loss: 0.3310 - val_accuracy: 0.9021\n",
      "Epoch 72/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1195 - accuracy: 0.9688 - val_loss: 0.3505 - val_accuracy: 0.8881\n",
      "Epoch 73/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1132 - accuracy: 0.9672 - val_loss: 0.3665 - val_accuracy: 0.9021\n",
      "Epoch 74/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1129 - accuracy: 0.9735 - val_loss: 0.2979 - val_accuracy: 0.9021\n",
      "Epoch 75/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1043 - accuracy: 0.9735 - val_loss: 0.2954 - val_accuracy: 0.9161\n",
      "Epoch 76/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1079 - accuracy: 0.9727 - val_loss: 0.2765 - val_accuracy: 0.9161\n",
      "Epoch 77/100\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.1067 - accuracy: 0.9719 - val_loss: 0.3366 - val_accuracy: 0.9161\n",
      "Epoch 78/100\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.1877 - accuracy: 0.9438 - val_loss: 0.4147 - val_accuracy: 0.8881\n",
      "Epoch 79/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.1398 - accuracy: 0.9610 - val_loss: 0.2706 - val_accuracy: 0.9091\n",
      "Epoch 80/100\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.1263 - accuracy: 0.9680 - val_loss: 0.2390 - val_accuracy: 0.9301\n",
      "Epoch 81/100\n",
      "41/41 [==============================] - 1s 28ms/step - loss: 0.1311 - accuracy: 0.9633 - val_loss: 0.3454 - val_accuracy: 0.8951\n",
      "Epoch 82/100\n",
      "41/41 [==============================] - 1s 27ms/step - loss: 0.1168 - accuracy: 0.9657 - val_loss: 0.3521 - val_accuracy: 0.8951\n",
      "Epoch 83/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1022 - accuracy: 0.9719 - val_loss: 0.3243 - val_accuracy: 0.9091\n",
      "Epoch 84/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.0948 - accuracy: 0.9758 - val_loss: 0.3260 - val_accuracy: 0.9161\n",
      "Epoch 85/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.0949 - accuracy: 0.9743 - val_loss: 0.2976 - val_accuracy: 0.9161\n",
      "Epoch 86/100\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.1198 - accuracy: 0.9657 - val_loss: 0.3397 - val_accuracy: 0.8951\n",
      "Epoch 87/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.0939 - accuracy: 0.9743 - val_loss: 0.2853 - val_accuracy: 0.9161\n",
      "Epoch 88/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.0959 - accuracy: 0.9750 - val_loss: 0.3349 - val_accuracy: 0.9021\n",
      "Epoch 89/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0881 - accuracy: 0.9797 - val_loss: 0.2869 - val_accuracy: 0.9161\n",
      "Epoch 90/100\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.0926 - accuracy: 0.9743 - val_loss: 0.2705 - val_accuracy: 0.9231\n",
      "Epoch 91/100\n",
      "41/41 [==============================] - 1s 28ms/step - loss: 0.1043 - accuracy: 0.9735 - val_loss: 0.2475 - val_accuracy: 0.9371\n",
      "Epoch 92/100\n",
      "41/41 [==============================] - 1s 26ms/step - loss: 0.0891 - accuracy: 0.9766 - val_loss: 0.2855 - val_accuracy: 0.9161\n",
      "Epoch 93/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0806 - accuracy: 0.9805 - val_loss: 0.2535 - val_accuracy: 0.9301\n",
      "Epoch 94/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0839 - accuracy: 0.9821 - val_loss: 0.2469 - val_accuracy: 0.9371\n",
      "Epoch 95/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0794 - accuracy: 0.9797 - val_loss: 0.2788 - val_accuracy: 0.9231\n",
      "Epoch 96/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.0777 - accuracy: 0.9782 - val_loss: 0.2679 - val_accuracy: 0.9371\n",
      "Epoch 97/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.0866 - accuracy: 0.9750 - val_loss: 0.2483 - val_accuracy: 0.9371\n",
      "Epoch 98/100\n",
      "41/41 [==============================] - 1s 25ms/step - loss: 0.0816 - accuracy: 0.9782 - val_loss: 0.3550 - val_accuracy: 0.9021\n",
      "Epoch 99/100\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.1191 - accuracy: 0.9657 - val_loss: 0.3201 - val_accuracy: 0.9161\n",
      "Epoch 100/100\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.1145 - accuracy: 0.9618 - val_loss: 0.4165 - val_accuracy: 0.8741\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3858 - accuracy: 0.8742\n",
      "Test error: 0.38580402731895447, Test accuracy: 0.8742138147354126\n"
     ]
    }
   ],
   "source": [
    "# Calling main function of object english_Model_Object\n",
    "english_Model_Object.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling main function of object nepali_Model_Object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Prediction Using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing Library necessary to predict our model\n",
    "\"\"\"\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as sw\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy import fftpack\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "import io\n",
    "import subprocess\n",
    "import wavefile\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.io.wavfile import write\n",
    "import sounddevice as sd\n",
    "from playsound import playsound\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Started: \n",
      "Say Now: \n",
      "numa\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "shutdown\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "shutdown\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "numa\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "google\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "open\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "google\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "shutdown\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "close\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "numa\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "close\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "shutdown\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "close\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "numa\n",
      "Enter S or s to stop: \n",
      "Say Now: \n",
      "shutdown\n",
      "Enter S or s to stop: s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Making prediction using english model we create.\n",
    "\n",
    ":param fps: frame per second.\n",
    ":param duaration : Record time duration.\n",
    ":param filename : audio path.\n",
    ":param mapping_Data : Loding the data to compare with our real time audio.\n",
    "\"\"\"\n",
    "\n",
    "fps = 44100\n",
    "duration = 1\n",
    "filename = \"prediction.wav\"\n",
    "mapping_Data = data[\"mappings\"]\n",
    "\n",
    "# English keyword spotting model\n",
    "model = load_model(\"/home/atomyongya/Documents/Herald/Final Year Project/VoiceAssistant(Numa)/VoiceAssistant/_system_Model/2_English_KM/3_English_Model_File/english_Model.h5\")\n",
    "\n",
    "print(\"Prediction Started: \")\n",
    "while True:\n",
    "    \n",
    "    \"\"\"\n",
    "    :param myrecording :  Audio to predict real time user voice.\n",
    "    :param prediction :  Prediction of real time audio voice.\n",
    "    :param predicted_index : Hold the max prediction value of our model.\n",
    "    :param predicted_keyword : Text word with which our voice will get compared. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Real time audio recording.  \n",
    "        print(\"Say Now: \")\n",
    "        myrecording = sd.rec(int(duration * fps), samplerate=fps, channels=2)\n",
    "        sd.wait()\n",
    "        write(filename, fps, myrecording)\n",
    "        \n",
    "        # Removing noise.\n",
    "        rate, reduced_data = sw.read('prediction.wav') \n",
    "        noise_Reduce = nr.reduce_noise(np.reshape(reduced_data, (2, -1)), rate)\n",
    "        \n",
    "        write(\"noise_Reduce.wav\", fps, reduced_data)\n",
    "        \n",
    "        # Loading the recorded file using librosa.\n",
    "        signal, sample_rate = librosa.load('prediction.wav')\n",
    "        \n",
    "        # Extracting the MFCC feature of an audio\n",
    "        mfcc = librosa.feature.mfcc(signal, sample_rate, n_mfcc=13, hop_length=512, n_fft=2048)\n",
    "        \n",
    "        # Making prediction and comparing our audio mfcc with the mfcc of train audio data\n",
    "        prediction = model.predict(tf.expand_dims(mfcc.T, axis=0))\n",
    "        \n",
    "        # Finding max prediction value and mapping with the index of mapping_Data from json. \n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_keyword = mapping_Data[predicted_index]\n",
    "        print(predicted_keyword)\n",
    "        \n",
    "        # To stop the audio record.\n",
    "        stop = input(\"Enter S or s to stop: \")\n",
    "        if stop == \"s\" or stop == \"S\":\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say Now: \n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile as wav\n",
    "from scipy.io.wavfile import write\n",
    "import sounddevice as sd\n",
    "from playsound import playsound\n",
    "import noisereduce as nr\n",
    "\n",
    "fs = 44100  # Sample rate\n",
    "seconds = 2   # Duration of recording\n",
    "print(\"Say Now: \")\n",
    "myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "write('prediction.wav', fs, myrecording)\n",
    "\n",
    "rate, data = sw.read('prediction.wav') \n",
    "reduced_noise = nr.reduce_noise(np.reshape(data, (2, -1)), rate)\n",
    "\n",
    "write(\"reduced_noise.wav\", fs, data)\n",
    "playsound(\"reduced_noise.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
